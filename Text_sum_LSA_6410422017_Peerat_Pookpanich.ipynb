{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeeratPookpanich/text_summarization_hw/blob/main/Text_sum_LSA_6410422017_Peerat_Pookpanich.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DADS7203 Natural Language Processing\n",
        "##6410422017 Peerat Pookpanich"
      ],
      "metadata": {
        "id": "MXbldqpptf2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Text pre-processing"
      ],
      "metadata": {
        "id": "aEoojRtRlzyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i4z9_9Zl6fp",
        "outputId": "27f0315e-efb8-49f6-a5f1-909de93ed4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Text Document"
      ],
      "metadata": {
        "id": "c9Nk84t6mBPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/PeeratPookpanich/text_summarization_hw/main/Transitioning%20from%20engineering%20to%20data%20science%20in%20Bangkok%20Thailand.txt\"\n",
        "article_content = requests.get(url)\n",
        "article_content = article_content.text\n",
        "print(article_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71pGZH55rux-",
        "outputId": "04ebd860-5910-4658-f0d3-ce770a360fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data science is still the most attractive trend in Thailand nowadays. \r\n",
            "There are many open positions for data scientists, data engineers, and data-related roles compared to 2 years ago. \r\n",
            "I am one of the people who jumped into this field as well. I want to share with you guys my journey of switching into this field.\r\n",
            "\r\n",
            "The decision\r\n",
            "\r\n",
            "In 2017, I was a mechanical engineer who worked in the petrochemical factory in Rayong province, Thailand. \r\n",
            "Typically, I was making a presentation to the management team about why the machine failed or how to prevent the machine breakdown. \r\n",
            "At that time, I had a chance to utilize the data to ensure and answer most technical questions.\r\n",
            "\r\n",
            "One day, I realized that Rayong is not a place for me to settle down. \r\n",
            "I wanted to go back to Bangkok, so I need to find a way to relocate myself. \r\n",
            "I was searching for a job, and it was hard to find a good one because my background is in mechanical engineering, and it is not that many good engineering jobs were placed in Bangkok. \r\n",
            "As a result, I decide to switch from the engineering field to the data field.\r\n",
            "\r\n",
            "Getting started\r\n",
            "\r\n",
            "Firstly, I had gathered information about this field, which is quite new in Thailand at that time. \r\n",
            "I also started to take an online course to see whether the subject was the right choice for me to move further. \r\n",
            "Fortunately, I took the most famous machine learning course, “Machine learning” from Standford university. \r\n",
            "The course was taught by Andrew Ng (Founder of the Coursera platform). \r\n",
            "Due to the superb explanation from Andrew Ng, every complex thing seem easy to follow and made much fun at the same time. \r\n",
            "I decided to take several courses after that because I feel enjoyable in this new field.\r\n",
            "\r\n",
            "Secondly, I entered the platform called “Kaggle.” That was a real starting point towards the data science field for me. \r\n",
            "I enrolled in many competitions and learned how to write the data analytics code through the kernel. \r\n",
            "The kernel was a good resource for a newbie to learn as much as you want. \r\n",
            "Several kinds of kernels accelerate your data skills, such as Exploratory data analysis (EDA), Modeling, Ensemble, Stacking, and other special data processing tips and tricks.\r\n",
            "\r\n",
            "Also, I took a master's degree in applied statistics during the transition phase. \r\n",
            "I hope that a solid academic background can help me in landing the new opportunity. \r\n",
            "However, It is not that much importance as I expected to land the data science job. \r\n",
            "I would recommend anyone interested in this field to go for the Kaggle use cases or make it on your own. \r\n",
            "The solid use cases have a lot of importance here in convincing your interviewer.\r\n",
            "\r\n",
            "How long does it take?\r\n",
            "\r\n",
            "It took me about 8 to 9 months of studying and exploring the Kaggle platform before applying for a data scientist position. \r\n",
            "That was around the middle of 2018. In Thailand, there were not many open data scientist positions. \r\n",
            "Most of the analytics jobs remain in the Business intelligence and Analyst field. \r\n",
            "I thought Linkedin is a beneficial tool for you here in breaking into this field. \r\n",
            "In Thailand, many information technologies or data positions have been placed on Linkedin, and the recruiter is very active in finding the candidate for the job. \r\n",
            "I really recommend you guys to have it professionally.\r\n",
            "\r\n",
            "After submitted a lot of application, I interviewed several companies ranged from start-up to a limited company. \r\n",
            "I was lucky to have a chance to interview in the largest bank in Thailand, and I nailed it with the data scientist position. \r\n",
            "The Kaggle competition helps me a lot here in getting my first data scientist job because there was an interview test called “Data challenge” that you have a limited time to do the analytics/modeling tasks and present to the interviewer at the end of the day. \r\n",
            "The pattern and best practices I got from the Kaggle competition made me so confident and comfortable.\r\n",
            "\r\n",
            "What I’ve done so far\r\n",
            "\r\n",
            "Since then, I’ve been working as a data scientist/analyst for around 3 years. \r\n",
            "There are many areas I have been involved in, for example, lending, investment, insurance, hospitality, and commercial building, etc. \r\n",
            "The benefit of being a data scientist is that there are many chances to touch many business operations areas. On the other hand, the downside is that data is never cleaned. \r\n",
            "We have to spend most of the time sanitizing, exploring, understanding to extract meaningful insight. That makes me sad sometimes but the time spending worth the benefit of the analysis. \r\n",
            "It is so overwhelming when you see the metrics go up in the A/B testing period.\r\n",
            "\r\n",
            "The difference\r\n",
            "Compared to when I learned from the online course and taking the Kaggle competition. \r\n",
            "I want to point out the difference between before and after I got the job.\r\n",
            "\r\n",
            "The modeling part is the most exciting and enjoyable part of all the data science tasks, but the time I have spent on it is at most 20–30 % compared to the other tasks.\r\n",
            "\r\n",
            "Like everyone pointed out in the data science blog, the most time-consumed task is data cleaning, and I would like to expand the scope to data understanding and correction. \r\n",
            "All the pretty data frame that I have seen from the Kaggle competition is not what exists in the real-life scenario. \r\n",
            "I need to collaborate with many parties to reach the truth about the data we have to analyze. There are many things to be verified before we can make a decision based on that data. \r\n",
            "It is not like the Kaggle competition that we have provided completed meta-data to read. Sometimes, nobody knows the valid and exact number of the operation process because there never glance at the data at all.\r\n",
            "\r\n",
            "The second time-consumed task is the back and forth process between the data analytics team and the business operating unit. \r\n",
            "A lot of meetings and communication play a vital part here. You know that the requirement can be changed on the fly based on the person you present the analysis to. \r\n",
            "If they have a high position enough, they can request to add anything based on their preference. \r\n",
            "You have to convince and recommend them wisely so that we will go through the implementation phase smoothly.\r\n",
            "\r\n",
            "Knowing this difference could help you guys recalibrate the expectation of doing the data science job. \r\n",
            "I heard many people complaining about the scope of work that it is not that fun as they are expected or how long they have to wait before they can do the modeling. \r\n",
            "I hope this makes you get a clearer vision of data science daily life.\r\n",
            "\r\n",
            "What is next\r\n",
            "\r\n",
            "Now, The data field in Thailand has grown so much compared to 2 years ago. \r\n",
            "A lot of people have delivered their analytics use cases to the in-house business. \r\n",
            "The management team starts to realize the benefit of having the data team inside their own organization. \r\n",
            "Even if some have a misunderstanding about what the data can do, it is normal for those guys who are not in the technical fields.\r\n",
            "We need to keep telling them the right way to utilize the data and its importance for the business.\r\n",
            "\r\n",
            "For myself, I still enjoy with the day to day data operation and also look forward to expanding my expertise towards the data engineering discipline. \r\n",
            "When you know what the model could do, it is like you have magical tools to impact selling/recommending the products. \r\n",
            "However, it needs the correct data to be fed into the model to have a promising prediction result. \r\n",
            "Data engineering is the answer here to control the quality and integrity of the data pipeline. \r\n",
            "I hope this would be the right choice that leads me to the bright future ahead in this field.\r\n",
            "\r\n",
            "I hope this encourages you guys who are interested in switching their job/responsibility towards this field. \r\n",
            "I think this industry still needs many people and the community to make sustainable growth throughout the disruption. Let’s join and make the impact together.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "article_content = re.sub(r'\\n|\\r', ' ', article_content)\n",
        "article_content = re.sub(r' +', ' ', article_content)\n",
        "article_content = article_content.strip()"
      ],
      "metadata": {
        "id": "V06iaktlmIM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(article_content)\n",
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXk_I0fVmY4X",
        "outputId": "4d86306f-5b6b-4602-85da-9c1a56256dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_sentences = normalize_corpus(sentences)\n",
        "norm_sentences[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz7jc7mgl1qm",
        "outputId": "7b388ed9-933a-4813-bcd6-80ff20400caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['data science still attractive trend thailand nowadays',\n",
              "       'many open positions data scientists data engineers datarelated roles compared years ago',\n",
              "       'one people jumped field well'], dtype='<U163')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
        "dt_matrix = tv.fit_transform(norm_sentences)\n",
        "dt_matrix = dt_matrix.toarray()\n",
        "\n",
        "vocab = tv.get_feature_names_out()\n",
        "td_matrix = dt_matrix.T\n",
        "print(td_matrix.shape)\n",
        "pd.DataFrame(np.round(td_matrix, 2), index=vocab).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "QBBuMpHGmj3p",
        "outputId": "1273661b-8f82-4240-a5ab-3d07ce467eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(399, 75)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0     1    2    3    4    5    6    7    8    9   ...   65   66  \\\n",
              "ab          0.0  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "academic    0.0  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "accelerate  0.0  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "active      0.0  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "add         0.0  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "ago         0.0  0.31  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "ahead       0.0  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "also        0.0  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "analysis    0.0  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "analyst     0.0  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
              "\n",
              "              67   68   69   70    71   72   73   74  \n",
              "ab          0.00  0.0  0.0  0.0  0.00  0.0  0.0  0.0  \n",
              "academic    0.00  0.0  0.0  0.0  0.00  0.0  0.0  0.0  \n",
              "accelerate  0.00  0.0  0.0  0.0  0.00  0.0  0.0  0.0  \n",
              "active      0.00  0.0  0.0  0.0  0.00  0.0  0.0  0.0  \n",
              "add         0.00  0.0  0.0  0.0  0.00  0.0  0.0  0.0  \n",
              "ago         0.00  0.0  0.0  0.0  0.00  0.0  0.0  0.0  \n",
              "ahead       0.00  0.0  0.0  0.0  0.37  0.0  0.0  0.0  \n",
              "also        0.23  0.0  0.0  0.0  0.00  0.0  0.0  0.0  \n",
              "analysis    0.00  0.0  0.0  0.0  0.00  0.0  0.0  0.0  \n",
              "analyst     0.00  0.0  0.0  0.0  0.00  0.0  0.0  0.0  \n",
              "\n",
              "[10 rows x 75 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d22bd99-fe40-4a7c-974e-b0a4e694b014\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ab</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>academic</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accelerate</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>active</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>add</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ago</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ahead</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>also</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>analysis</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>analyst</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 75 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d22bd99-fe40-4a7c-974e-b0a4e694b014')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d22bd99-fe40-4a7c-974e-b0a4e694b014 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d22bd99-fe40-4a7c-974e-b0a4e694b014');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Semantic Analysis\n",
        "\n",
        "Apply low-rank Singular Value Decomposition to this matrix. The core principle behind Latent Semantic Analysis (LSA) is that in any document, there exists a latent structure among terms that are related\n",
        "contextually and hence should also be correlated in the same singular space.\n",
        "\n",
        "The main idea in our implementation is to use SVD (recall M = USVT) so that U\n",
        "and V are the orthogonal matrices and S is the diagonal matrix, which can also be\n",
        "represented as a vector of the singular values. \n",
        "\n",
        "The original matrix can be represented as\n",
        "a term-document matrix where the rows are terms and each column is a document, i.e.,\n",
        "a sentence from our document in this case. The values can be any type of weighting like\n",
        "Bag of Words model-based frequencies, TF-IDFs, or binary occurrences.\n",
        "\n",
        "![](https://i.imgur.com/YtopNr3.png)"
      ],
      "metadata": {
        "id": "34cYo2oUncAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "    \n",
        "def low_rank_svd(matrix, singular_count=2):\n",
        "    u, s, vt = svds(matrix, k=singular_count)\n",
        "    return u, s, vt"
      ],
      "metadata": {
        "id": "nJjbq2__nsF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sentences = 8\n",
        "num_topics = 3\n",
        "\n",
        "u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)  \n",
        "print(u.shape, s.shape, vt.shape)\n",
        "term_topic_mat, singular_values, topic_document_mat = u, s, vt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiAzeyCknwKk",
        "outputId": "cb0c7bad-9e3a-414c-fcaa-b87bd5bf3d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(399, 3) (3,) (3, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove singular values below threshold                                         \n",
        "sv_threshold = 0.5\n",
        "min_sigma_value = max(singular_values) * sv_threshold\n",
        "singular_values[singular_values < min_sigma_value] = 0"
      ],
      "metadata": {
        "id": "SqYrvWMeoF4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salience_scores = np.sqrt(np.dot(np.square(singular_values), \n",
        "                                 np.square(topic_document_mat)))\n",
        "salience_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHakgfQloIt_",
        "outputId": "c6a3cd1c-2d4c-48ac-a1b0-f1ebdcb3ce4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.33492016, 0.52500692, 0.37348266, 0.47469627, 0.14054693,\n",
              "       0.03439453, 0.23871801, 0.0787093 , 0.20686297, 0.23948814,\n",
              "       0.44722253, 0.25374833, 0.17496657, 0.07736326, 0.16249226,\n",
              "       0.12176427, 0.2852343 , 0.45772019, 0.29694581, 0.14007367,\n",
              "       0.25993774, 0.04835826, 0.18155796, 0.33731336, 0.55012501,\n",
              "       0.37296844, 0.09458874, 0.32731158, 0.01812493, 0.6605594 ,\n",
              "       0.41860437, 0.2310217 , 0.39845551, 0.25243084, 0.09335506,\n",
              "       0.35269007, 0.41065046, 0.31705163, 0.14529888, 0.19857846,\n",
              "       0.52612016, 0.17893873, 0.09710242, 0.10334108, 0.09583107,\n",
              "       0.42323164, 0.39859951, 0.27528109, 0.32500898, 0.30583542,\n",
              "       0.31292114, 0.35921656, 0.30286234, 0.14607397, 0.29765589,\n",
              "       0.0878497 , 0.06084374, 0.06686582, 0.17060446, 0.42001863,\n",
              "       0.22537105, 0.22284131, 0.38907552, 0.55180705, 0.17343551,\n",
              "       0.21005421, 0.30714613, 0.26765359, 0.10369973, 0.16297536,\n",
              "       0.30081344, 0.34964022, 0.51733889, 0.26680969, 0.09080815])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_sentence_indices = (-salience_scores).argsort()[:num_sentences]\n",
        "top_sentence_indices.sort()"
      ],
      "metadata": {
        "id": "24uQ4vSioLwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n'.join(np.array(sentences)[top_sentence_indices]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0Iej-kFoNxF",
        "outputId": "813b2172-0413-40ee-b8f0-04c5045dd136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are many open positions for data scientists, data engineers, and data-related roles compared to 2 years ago.\n",
            "I want to share with you guys my journey of switching into this field.\n",
            "Secondly, I entered the platform called “Kaggle.” That was a real starting point towards the data science field for me.\n",
            "I would recommend anyone interested in this field to go for the Kaggle use cases or make it on your own.\n",
            "In Thailand, there were not many open data scientist positions.\n",
            "The benefit of being a data scientist is that there are many chances to touch many business operations areas.\n",
            "A lot of people have delivered their analytics use cases to the in-house business.\n",
            "I hope this encourages you guys who are interested in switching their job/responsibility towards this field.\n"
          ]
        }
      ]
    }
  ]
}